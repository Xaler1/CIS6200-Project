{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.652954600Z",
     "start_time": "2024-05-06T01:43:32.605702300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
    "import lpips\n",
    "from pytorch_msssim import ssim\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import gc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_folder = './test_split'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.655463900Z",
     "start_time": "2024-05-06T01:43:46.626432300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "grayscale_transform = transforms.Compose([\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class GrayscaleDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None, grayscale_transform=None, extra_prompt=False):\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self.grayscale_transform = grayscale_transform\n",
    "        self.extra_prompt = extra_prompt\n",
    "        all_files = os.listdir(data_folder)\n",
    "        self.ids = list(set(file.split('_')[0] for file in all_files if file.split('_')[0].isdigit()))\n",
    "        print(\"Collected IDs:\", self.ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.ids[idx]\n",
    "\n",
    "        image_path = os.path.join(self.data_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        gray_image_path = os.path.join(self.data_folder, f\"{image_id}_gray.jpg\")\n",
    "        gray_image = Image.open(gray_image_path)\n",
    "\n",
    "        mask_path = os.path.join(self.data_folder, f\"{image_id}_mask.jpg\")\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        text_path = os.path.join(self.data_folder, f\"{image_id}.txt\")\n",
    "        with open(text_path, 'r') as file:\n",
    "            text_data = file.read().strip()\n",
    "\n",
    "        if self.extra_prompt:\n",
    "            text_data = \"Colorize the whole image. \" + text_data\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            gray_image = self.grayscale_transform(gray_image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "\n",
    "        return {'image': image, 'input_image': gray_image, 'gray_image': gray_image, 'mask': mask, 'text': text_data}\n",
    "\n",
    "\n",
    "class CannyDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None, grayscale_transform=None, extra_prompt=False):\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self.grayscale_transform = grayscale_transform\n",
    "        all_files = os.listdir(data_folder)\n",
    "        self.ids = list(set(file.split('_')[0] for file in all_files if file.split('_')[0].isdigit()))\n",
    "        print(\"Collected IDs:\", self.ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.ids[idx]\n",
    "\n",
    "        image_path = os.path.join(self.data_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        gray_image_path = os.path.join(self.data_folder, f\"{image_id}_gray.jpg\")\n",
    "        gray_image = Image.open(gray_image_path)\n",
    "        canny_image = np.array(gray_image)\n",
    "\n",
    "        low_threshold = 100\n",
    "        high_threshold = 200\n",
    "        canny_image = cv2.Canny(canny_image, low_threshold, high_threshold)\n",
    "        canny_image = canny_image[:, :, None]\n",
    "        canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "        canny_image = Image.fromarray(canny_image)\n",
    "\n",
    "        mask_path = os.path.join(self.data_folder, f\"{image_id}_mask.jpg\")\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        text_path = os.path.join(self.data_folder, f\"{image_id}.txt\")\n",
    "        with open(text_path, 'r') as file:\n",
    "            text_data = file.read().strip()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            canny_image = self.transform(canny_image)\n",
    "            gray_image = self.grayscale_transform(gray_image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "\n",
    "        return {'image': image, 'input_image': canny_image, 'gray_image': gray_image, 'mask': mask, 'text': text_data}\n",
    "\n",
    "\n",
    "\n",
    "def get_loader(grayscale, extra_prompt=False):\n",
    "    if grayscale:\n",
    "        dataset = GrayscaleDataset(data_folder, transform=transform, grayscale_transform=grayscale_transform, extra_prompt=extra_prompt)\n",
    "    else:\n",
    "        dataset = CannyDataset(data_folder, transform=transform, grayscale_transform=grayscale_transform)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    return loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.655964Z",
     "start_time": "2024-05-06T01:43:46.653455600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_pipe(grayscale=False):\n",
    "    if grayscale:\n",
    "        controlnet = ControlNetModel.from_pretrained(\"latentcat/control_v1p_sd15_brightness\", use_safetensors=True)\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\n",
    "    else:\n",
    "        controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\n",
    "    pipe.safety_checker = lambda images, clip_input: (images, [False] * len(images))\n",
    "    return pipe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.670968500Z",
     "start_time": "2024-05-06T01:43:46.657962100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def brighten_image(image, brightness=100):\n",
    "    # Make sure the brightness value is appropriate to prevent overflow\n",
    "    brightness = np.clip(brightness, 0, 255)\n",
    "\n",
    "    # Create an array of the same shape as the image, filled with the brightness value\n",
    "    brightness_matrix = np.ones(image.shape, dtype=np.uint8) * brightness\n",
    "\n",
    "    # Add the brightness matrix to the image\n",
    "    brightened_image = cv2.add(image, brightness_matrix)\n",
    "\n",
    "    return brightened_image\n",
    "\n",
    "\n",
    "def postprocess(grayscale, colored):\n",
    "    grayscale = grayscale[0]\n",
    "\n",
    "    # Convert the grayscale image to Lab color space\n",
    "    grayscale = cv2.cvtColor(grayscale, cv2.COLOR_GRAY2BGR)\n",
    "    grayscale_lab = cv2.cvtColor(grayscale, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "    # Convert the colored image to Lab color space\n",
    "    colored = cv2.cvtColor(colored, cv2.COLOR_RGB2BGR)\n",
    "    colored_lab = cv2.cvtColor(colored, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "    # Replace the 'a' and 'b' channels of the grayscale image with the ones from the colored image\n",
    "    combined_lab = np.concatenate((grayscale_lab[:, :, 0:1], colored_lab[:, :, 1:]), axis=2)\n",
    "\n",
    "    # Convert the result back to RGB color space\n",
    "    colorized = cv2.cvtColor(combined_lab.astype('uint8'), cv2.COLOR_Lab2RGB)\n",
    "\n",
    "    return colorized\n",
    "\n",
    "def adjust_intensity(gray, colored):\n",
    "    target = np.transpose(gray, (1, 2, 0))\n",
    "    source = colored\n",
    "\n",
    "    # Calculate the grayscale intensity of the source and target images\n",
    "    source_intensity = 0.299 * source[..., 0] + 0.587 * source[..., 1] + 0.114 * source[..., 2]\n",
    "    target_intensity = 0.299 * target[..., 0] + 0.587 * target[..., 1] + 0.114 * target[..., 2]\n",
    "\n",
    "    # Calculate the adjustment factor\n",
    "    k = target_intensity / source_intensity\n",
    "\n",
    "    # Adjust the RGB values\n",
    "    adjusted = source * k[..., np.newaxis]\n",
    "\n",
    "    # Clip the values to the range [0, 255]\n",
    "    adjusted = np.clip(adjusted, 0, 255).astype('uint8')\n",
    "\n",
    "    return adjusted\n",
    "\n",
    "def color_transfer(gray, colored):\n",
    "    target = np.transpose(gray, (1, 2, 0))\n",
    "    source = colored\n",
    "\n",
    "    # Convert the source and target images to float32\n",
    "    source = source.astype('float32')\n",
    "    target = target.astype('float32')\n",
    "\n",
    "    # Split the source and target images into their respective color channels\n",
    "    source_b, source_g, source_r = cv2.split(source)\n",
    "    target_b, target_g, target_r = cv2.split(target)\n",
    "\n",
    "    # Calculate the mean and standard deviation of each color channel in the source and target images\n",
    "    source_mean, source_std = np.mean(source_b), np.std(source_b)\n",
    "    target_mean, target_std = np.mean(target_b), np.std(target_b)\n",
    "\n",
    "    # Normalize each color channel of the source image by subtracting the mean and dividing by the standard deviation\n",
    "    source_b = (source_b - source_mean) / source_std\n",
    "\n",
    "    # Scale and shift each color channel of the target image using the mean and standard deviation of the source image\n",
    "    target_b = target_b * source_std + source_mean\n",
    "\n",
    "    # Repeat the above steps for the green and red color channels\n",
    "    source_mean, source_std = np.mean(source_g), np.std(source_g)\n",
    "    target_mean, target_std = np.mean(target_g), np.std(target_g)\n",
    "    source_g = (source_g - source_mean) / source_std\n",
    "    target_g = target_g * source_std + source_mean\n",
    "\n",
    "    source_mean, source_std = np.mean(source_r), np.std(source_r)\n",
    "    target_mean, target_std = np.mean(target_r), np.std(target_r)\n",
    "    source_r = (source_r - source_mean) / source_std\n",
    "    target_r = target_r * source_std + source_mean\n",
    "\n",
    "    # Merge the color channels back together\n",
    "    transfer = cv2.merge([target_b, target_g, target_r])\n",
    "\n",
    "    # Clip the values in the transfer image to the range [0, 255] and convert it back to uint8\n",
    "    transfer = np.clip(transfer, 0, 255).astype('uint8')\n",
    "\n",
    "    return transfer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.692504100Z",
     "start_time": "2024-05-06T01:43:46.679978500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def calculate_metrics(generated, original, mask):\n",
    "\n",
    "    if not isinstance(generated, torch.Tensor):\n",
    "        generated = to_tensor(generated).unsqueeze(0).to('cuda')\n",
    "    if not isinstance(original, torch.Tensor):\n",
    "        original = to_tensor(original).unsqueeze(0).to('cuda')\n",
    "    if not isinstance(mask, torch.Tensor):\n",
    "        mask = to_tensor(mask).unsqueeze(0).to('cuda')\n",
    "\n",
    "    if generated.dim() == 3:\n",
    "        generated = generated.unsqueeze(0)\n",
    "    if original.dim() == 3:\n",
    "        original = original.unsqueeze(0)\n",
    "    if mask.dim() == 3:\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "    if generated.shape[1] == 1:\n",
    "        generated = generated.repeat(1, 3, 1, 1)\n",
    "    if original.shape[1] == 1:\n",
    "        original = original.repeat(1, 3, 1, 1)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    full_mse = mse_loss(generated, original)\n",
    "\n",
    "    if mask.shape[1] != 1:\n",
    "        mask = mask[:, 0:1, :, :]\n",
    "\n",
    "    masked_generated = generated * mask\n",
    "    masked_original = original * mask\n",
    "    masked_mse = mse_loss(masked_generated, masked_original)\n",
    "\n",
    "    ssim_val = ssim(generated, original, data_range=1, size_average=True)\n",
    "    lpips_vgg = lpips.LPIPS(net='vgg').to('cuda')\n",
    "    lpips_val = lpips_vgg(generated, original)\n",
    "    return full_mse.item(), masked_mse.item(), ssim_val.item(), lpips_val.item()\n",
    "\n",
    "def run_inference(data_loader, model, guidance_scale=4.0, post=False):\n",
    "    results = []\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            texts = batch['text']\n",
    "            input_images = batch['input_image'].to('cuda')\n",
    "            gray_images = batch['gray_image']\n",
    "            gt_images = batch['image'].to('cuda')\n",
    "            masks = batch['mask'].to('cuda')\n",
    "            generated_images = model(prompt=texts, image=input_images, guess_mode=False, guidance_scale=guidance_scale).images\n",
    "\n",
    "            for gen_img, gt_img, input_img, gray_img, mask, text in zip(generated_images, gt_images, input_images, gray_images, masks, texts):\n",
    "                if post:\n",
    "                    gen_img = postprocess(gray_img.numpy(), np.array(gen_img))\n",
    "                metrics = calculate_metrics(gen_img, gt_img, mask)\n",
    "\n",
    "                if i < 10:\n",
    "                    print(metrics)\n",
    "                    i += 1\n",
    "\n",
    "                results.append((text, gen_img, gt_img, input_img, metrics))\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.715024100Z",
     "start_time": "2024-05-06T01:43:46.694005400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ids = [str(i) for i in range(1, 50)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T01:43:46.744552400Z",
     "start_time": "2024-05-06T01:43:46.703016900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pix2Pix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "os.makedirs(\"plotting\", exist_ok=True)\n",
    "\n",
    "for id in ids:\n",
    "    target = Image.open(f\"test_split/{id}.jpg\")\n",
    "    gray = Image.open(f\"test_split/{id}_gray.jpg\")\n",
    "    with open(f\"test_split/{id}.txt\", \"r\") as f:\n",
    "        prompt = f.readline()\n",
    "    prompt = \"Colorize the whole image. \" + prompt\n",
    "\n",
    "    generated = pipe(prompt, gray, guess_mode=False, guidance_scale=7.0).images[0]\n",
    "\n",
    "    generated.save(f\"plotting/pix2pix_{id}.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Brightness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "pipe = get_pipe(True)\n",
    "\n",
    "os.makedirs(\"plotting\", exist_ok=True)\n",
    "\n",
    "for id in ids:\n",
    "    target = Image.open(f\"test_split/{id}.jpg\")\n",
    "    gray = Image.open(f\"test_split/{id}_gray.jpg\")\n",
    "    with open(f\"test_split/{id}.txt\", \"r\") as f:\n",
    "        prompt = f.readline()\n",
    "\n",
    "    generated = pipe(prompt, gray, guess_mode=False, guidance_scale=4.0).images[0]\n",
    "\n",
    "    generated.save(f\"plotting/brightness_{id}.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ledits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "from diffusers import LEditsPPPipelineStableDiffusion\n",
    "pipe = LEditsPPPipelineStableDiffusion.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "os.makedirs(\"plotting\", exist_ok=True)\n",
    "\n",
    "for id in ids:\n",
    "    target = Image.open(f\"test_split/{id}.jpg\")\n",
    "    gray = Image.open(f\"test_split/{id}_gray.jpg\")\n",
    "    with open(f\"test_split/{id}.txt\", \"r\") as f:\n",
    "        prompt = f.readline()\n",
    "\n",
    "    _  = pipe.invert(gray, num_inversion_steps=50, skip=0.1)\n",
    "\n",
    "    generated = pipe(editing_prompt=[prompt], edit_guidance_scale=10.0, edit_threshold=0.75).images[0]\n",
    "\n",
    "    generated.save(f\"plotting/ledits_{id}.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"../../trained_model\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "os.makedirs(\"plotting\", exist_ok=True)\n",
    "\n",
    "for id in ids:\n",
    "    target = Image.open(f\"test_split/{id}.jpg\")\n",
    "    gray = Image.open(f\"test_split/{id}_gray.jpg\")\n",
    "    with open(f\"test_split/{id}.txt\", \"r\") as f:\n",
    "        prompt = f.readline()\n",
    "\n",
    "    generated = pipe(prompt, gray, guess_mode=False, guidance_scale=4.0).images[0]\n",
    "\n",
    "    generated.save(f\"plotting/v1_{id}.jpg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
